# AtaHub Carona - ETL System

Sistema completo de ETL (Extract, Transform, Load) para ingestÃ£o de dados de Atas de Registro de PreÃ§os (ARP) da API de Dados Abertos do Governo Federal.

## ğŸš€ CaracterÃ­sticas

- âœ… **Processamento AssÃ­ncrono** - asyncio/aiohttp para alta performance
- âœ… **Rate Limiting Inteligente** - Token bucket com 3 req/s + exponential backoff
- âœ… **ETL Incremental** - AtualizaÃ§Ãµes diÃ¡rias com lookback window de 7 dias
- âœ… **Checkpoint/Resume** - RecuperaÃ§Ã£o automÃ¡tica de falhas
- âœ… **Soft Deletes** - Preserva histÃ³rico de dados excluÃ­dos
- âœ… **Full-Text Search** - Busca eficiente em PostgreSQL com Ã­ndices GIN
- âœ… **Monitoramento** - Tracking de execuÃ§Ãµes, erros e estatÃ­sticas
- âœ… **Docker Ready** - Containerizado com Docker Compose
- âœ… **Scheduler Integrado** - APScheduler para execuÃ§Ã£o automÃ¡tica diÃ¡ria

## ğŸ“‹ PrÃ©-Requisitos

- Python 3.11+
- PostgreSQL 15+
- Docker e Docker Compose (opcional)

## ğŸ› ï¸ InstalaÃ§Ã£o

### 1. Instalar DependÃªncias

```bash
cd etl
pip install -r requirements.txt
```

### 2. Configurar Ambiente

Copie o arquivo de exemplo e configure:

```bash
cp .env.example .env
```

Edite `.env` com suas configuraÃ§Ãµes:

```bash
DATABASE_URL=postgresql://postgres:password@localhost:5433/govcompras
API_BASE_URL=https://dadosabertos.compras.gov.br
REQUESTS_PER_SECOND=3.0
INITIAL_LOAD_START_DATE=2023-01-01
```

### 3. Executar Migration do Banco

**IMPORTANTE:** FaÃ§a backup antes!

```bash
# Via psql
psql -U postgres -h localhost -p 5433 -d govcompras -f ../migrations/001_enhanced_schema.sql

# Via Docker
docker cp ../migrations/001_enhanced_schema.sql atahub_db:/tmp/
docker exec -it atahub_db psql -U postgres -d govcompras -f /tmp/001_enhanced_schema.sql
```

Consulte `../migrations/README.md` para mais detalhes.

## ğŸ¯ Uso

### Modo 1: CLI (Linha de Comando)

#### Carga Inicial

Processa dados dos Ãºltimos 2-3 anos:

```bash
# PerÃ­odo padrÃ£o (configurado no .env)
python run_initial_load.py

# PerÃ­odo personalizado
python run_initial_load.py --start 2023-01-01 --end 2024-12-31

# Modo de teste (pÃ¡ginas limitadas)
python run_initial_load.py --test

# Dry run (nÃ£o salva no banco)
python run_initial_load.py --dry-run
```

#### AtualizaÃ§Ã£o Incremental

Busca mudanÃ§as desde a Ãºltima sincronizaÃ§Ã£o:

```bash
python run_incremental.py
```

### Modo 2: Scheduler AutomÃ¡tico

Executa atualizaÃ§Ãµes incrementais diariamente Ã s 2 AM:

```bash
python scheduler.py
```

ConfiguraÃ§Ã£o no `.env`:

```bash
ETL_SCHEDULE_ENABLED=true
ETL_SCHEDULE_HOUR=2
ETL_SCHEDULE_MINUTE=0
ETL_SCHEDULE_TIMEZONE=America/Sao_Paulo
```

### Modo 3: Docker Compose

Inicia todo o stack incluindo ETL scheduler:

```bash
# Subir todos os serviÃ§os
docker-compose up -d

# Ver logs do ETL
docker-compose logs -f etl

# Executar carga inicial manualmente
docker-compose exec etl python run_initial_load.py

# Executar incremental manualmente
docker-compose exec etl python run_incremental.py
```

## ğŸ“Š Monitoramento

### Endpoints Admin API

O backend FastAPI expÃµe endpoints para monitoramento:

#### Status Atual

```bash
curl http://localhost:8000/admin/etl/status
```

Resposta:
```json
{
  "execution_id": "uuid",
  "status": "running",
  "progress": "15/282",
  "arps_processed": 7500,
  "items_processed": 45000,
  "errors": 12,
  "duration_seconds": 3600,
  "started_at": "2025-01-26T02:00:00"
}
```

#### HistÃ³rico de ExecuÃ§Ãµes

```bash
curl http://localhost:8000/admin/etl/executions?limit=10
```

#### Erros do ETL

```bash
curl http://localhost:8000/admin/etl/errors?limit=50
```

#### EstatÃ­sticas Gerais

```bash
curl http://localhost:8000/admin/etl/stats
```

Resposta:
```json
{
  "arps": {
    "total": 140000,
    "active": 138500,
    "valid": 95000
  },
  "items": {
    "total": 1250000,
    "active": 1230000
  },
  "executions": {
    "total": 45,
    "completed": 43,
    "failed": 2
  }
}
```

### Logs

Logs sÃ£o salvos em `logs/etl.log` (configurÃ¡vel):

```bash
# Ver logs em tempo real
tail -f logs/etl.log

# Buscar erros
grep "error" logs/etl.log
```

### Queries de Monitoramento

```sql
-- Status da Ãºltima execuÃ§Ã£o
SELECT * FROM etl_executions
ORDER BY started_at DESC LIMIT 1;

-- Erros nÃ£o resolvidos
SELECT * FROM etl_errors
WHERE resolved = FALSE;

-- EstatÃ­sticas de ARPs
SELECT
    COUNT(*) as total,
    COUNT(*) FILTER (WHERE ata_excluido = FALSE) as ativas,
    COUNT(*) FILTER (WHERE data_fim_vigencia >= CURRENT_DATE) as vigentes
FROM arps;
```

## ğŸ—ï¸ Arquitetura

```
etl/
â”œâ”€â”€ config.py              # ConfiguraÃ§Ã£o centralizada (Pydantic)
â”œâ”€â”€ database.py            # Database async (asyncpg + SQLAlchemy)
â”œâ”€â”€ models.py              # ORM models
â”œâ”€â”€ api_client.py          # Cliente HTTP async com rate limiting
â”œâ”€â”€ orchestrator.py        # Coordenador principal do ETL
â”‚
â”œâ”€â”€ processors/            # Processadores de dados
â”‚   â”œâ”€â”€ transformers.py    # Mapeamento API â†’ DB
â”‚   â”œâ”€â”€ arp_processor.py   # Processador de ARPs
â”‚   â””â”€â”€ item_processor.py  # Processador de itens
â”‚
â”œâ”€â”€ utils/                 # UtilitÃ¡rios
â”‚   â”œâ”€â”€ retry_utils.py     # Retry com backoff
â”‚   â””â”€â”€ date_utils.py      # ManipulaÃ§Ã£o de datas
â”‚
â”œâ”€â”€ run_initial_load.py    # CLI: carga inicial
â”œâ”€â”€ run_incremental.py     # CLI: incremental
â”œâ”€â”€ scheduler.py           # Scheduler (APScheduler)
â””â”€â”€ Dockerfile             # Container ETL
```

## âš™ï¸ ConfiguraÃ§Ãµes Importantes

### Rate Limiting

```bash
REQUESTS_PER_SECOND=3.0  # Conservador para API governamental
MAX_RETRIES=3
RETRY_BACKOFF_FACTOR=2.0
```

### Batch Sizes

```bash
PAGE_SIZE=500                    # MÃ¡ximo permitido pela API
BATCH_SIZE_ARPS=100             # ARPs por transaÃ§Ã£o
BATCH_SIZE_ITEMS=500            # Itens por bulk insert
MAX_CONCURRENT_ITEM_REQUESTS=5  # Requests simultÃ¢neos
```

### Datas

```bash
INITIAL_LOAD_START_DATE=2023-01-01
INCREMENTAL_LOOKBACK_DAYS=7  # Captura atualizaÃ§Ãµes tardias
```

## ğŸ“ˆ Performance

### Estimativas (2-3 anos de dados)

- **ARPs:** ~140.000 registros
- **Itens:** ~1-3 milhÃµes (10-20 itens/ARP)
- **Tempo Carga Inicial:** 24-48 horas
- **Tempo Incremental DiÃ¡rio:** 5-30 minutos
- **EspaÃ§o em Disco:** ~50-100 GB

### OtimizaÃ§Ãµes

- Processamento assÃ­ncrono (~70% mais rÃ¡pido que sÃ­ncrono)
- Bulk inserts (100x mais rÃ¡pido que individual)
- Ãndices GIN para full-text search
- Connection pooling (5 conexÃµes + 10 overflow)
- Checkpoint a cada 10 pÃ¡ginas

## ğŸ› Troubleshooting

### Erro: "permission denied"

```bash
# Verificar permissÃµes no PostgreSQL
psql -U postgres -d govcompras -c "GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO postgres;"
```

### Erro: "rate limited" (429)

Aguarde ou reduza `REQUESTS_PER_SECOND` no `.env`:

```bash
REQUESTS_PER_SECOND=2.0  # Mais conservador
```

### ETL travou/crashou

O sistema possui checkpoint/resume automÃ¡tico. Basta reiniciar:

```bash
python run_incremental.py
```

### Banco de dados lento apÃ³s carga

Execute VACUUM ANALYZE:

```sql
VACUUM ANALYZE arps;
VACUUM ANALYZE itens_arp;
```

### Ver progresso em tempo real

```bash
# Terminal 1: Logs
tail -f logs/etl.log

# Terminal 2: Status via API
watch -n 5 'curl -s http://localhost:8000/admin/etl/status | jq'

# Terminal 3: Contagem no banco
watch -n 10 'psql -U postgres -d govcompras -c "SELECT COUNT(*) FROM arps;"'
```

## ğŸ”’ SeguranÃ§a

- **Rate Limiting:** Protege contra sobrecarga da API governamental
- **Soft Deletes:** Preserva histÃ³rico, nÃ£o permite perda de dados
- **ValidaÃ§Ã£o de Dados:** Valida antes de inserir no banco
- **TransaÃ§Ãµes AtÃ´micas:** Rollback automÃ¡tico em erros
- **Error Tracking:** Dead letter queue para retry posterior

## ğŸ“ ManutenÃ§Ã£o

### Rotinas Recomendadas

**Semanal:**
- Verificar erros nÃ£o resolvidos
- Revisar logs para anomalias

**Mensal:**
- VACUUM ANALYZE nas tabelas grandes
- Revisar performance de queries
- Limpar logs antigos

**Trimestral:**
- Analisar query plans
- Ajustar Ã­ndices se necessÃ¡rio
- Revisar configuraÃ§Ãµes de rate limiting

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto faz parte do AtaHub Carona.

## ğŸ†˜ Suporte

Para problemas ou dÃºvidas:

1. Verifique os logs: `logs/etl.log`
2. Consulte o troubleshooting acima
3. Verifique status via API: `/admin/etl/status`
4. Abra uma issue no repositÃ³rio

---

**Desenvolvido com â¤ï¸ para facilitar o acesso a dados de compras governamentais**
